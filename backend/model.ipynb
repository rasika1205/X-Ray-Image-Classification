{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b52605d",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22690da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61893781",
   "metadata": {},
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0340e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES=['NORMAL','PNEUMONIA']\n",
    "DataDir=r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\train'\n",
    "training_data=[]\n",
    "img_size=100\n",
    "def create_training_data():\n",
    "    for i in CATEGORIES:\n",
    "\n",
    "        path=os.path.join(DataDir,i)\n",
    "        class_num=CATEGORIES.index(i)\n",
    "\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                new_array=cv2.resize(img_array,(img_size,img_size))\n",
    "                training_data.append([new_array,class_num])\n",
    "            \n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_training_data()\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "X=[]\n",
    "y=[]\n",
    "# spliting the features and labels\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "y = np.array(y)\n",
    "X = np.array(X).reshape(-1,img_size,img_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data=[]\n",
    "DataDir_val= r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\val'\n",
    "def create_validating_data():\n",
    "    for i in CATEGORIES:\n",
    "\n",
    "        path=os.path.join(DataDir_val,i)\n",
    "        class_num=CATEGORIES.index(i)\n",
    "\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                new_array=cv2.resize(img_array,(img_size,img_size))\n",
    "                validation_data.append([new_array,class_num])\n",
    "            \n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_validating_data()\n",
    "import random\n",
    "random.shuffle(validation_data)\n",
    "X_val=[]\n",
    "y_val=[]\n",
    "for features,label in validation_data:\n",
    "    X_val.append(features)\n",
    "    y_val.append(label)\n",
    "y_val = np.array(y_val)\n",
    "X_val=np.array(X_val).reshape(-1,img_size,img_size,1)\n",
    "X = X/255.0\n",
    "x_val = X_val/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11778dd6",
   "metadata": {},
   "source": [
    "### 3. Build CNN Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e10cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Conv2D(256,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history=model.fit(X,y,batch_size=4,epochs=10,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf93ff",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14401841",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\test'\n",
    "# define categories\n",
    "CATEGORIES = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "\n",
    "# set image size\n",
    "img_size = 100\n",
    "\n",
    "# initialize lists for storing test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# loop through the test data directory and extract the images and their labels\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(test_dir, category)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "            X_test.append(new_array)\n",
    "            y_test.append(class_num)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# convert test data to numpy arrays\n",
    "X_test = np.array(X_test).reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# normalize test data\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# calculate test accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38a7fb",
   "metadata": {},
   "source": [
    "20/20 - 4s - 178ms/step - accuracy: 0.8590 - loss: 0.7382\n",
    "Test accuracy: 0.8589743375778198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training & Validation Metrics\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba521c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_val, y_pred_classes, target_names=['Normal', 'Pneumonia']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304d4df",
   "metadata": {},
   "source": [
    "### CNN From Scratch:\n",
    "-  ✔ Validation Accuracy: ~0.9375\n",
    "-  ✔ Test Accuracy: ~0.8589"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622b342",
   "metadata": {},
   "source": [
    "### 5. Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024227a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Conv Layer 1\n",
    "    model.add(Conv2D(\n",
    "        filters=64,  # fixed here; tune filters later if you want\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(100, 100, 1)\n",
    "    ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Conv Layer 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Conv Layer 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Conv Layer 4\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Choice('dense_units', [64, 128]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    # Dropout\n",
    "    model.add(Dropout(hp.Float('dropout_rate', 0.3, 0.5, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4, 5e-4])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "class MyTuner(RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "\n",
    "        # Tune batch size and epochs here\n",
    "        kwargs['batch_size'] = hp.Choice('batch_size', [4, 8, 16])\n",
    "        kwargs['epochs'] = hp.Choice('epochs', [10, 15, 20])\n",
    "\n",
    "        return super().run_trial(trial, *args, **kwargs)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tuner = MyTuner(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='hpo_dir',\n",
    "    project_name='pneumonia_cnn_batchsize_epochs'\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "tuner.search(X, y,\n",
    "             validation_data=(x_val, y_val),\n",
    "             callbacks=[early_stop])\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"✅ Best Hyperparameters:\")\n",
    "print(f\"Batch Size: {best_hp.get('batch_size')}\")\n",
    "print(f\"Epochs: {best_hp.get('epochs')}\")\n",
    "print(f\"Learning Rate: {best_hp.get('learning_rate')}\")\n",
    "print(f\"Dense Units: {best_hp.get('dense_units')}\")\n",
    "print(f\"Dropout: {best_hp.get('dropout_rate')}\")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = best_model.evaluate(x_val, y_val)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\test'\n",
    "# define categories\n",
    "CATEGORIES = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "\n",
    "# set image size\n",
    "img_size = 100\n",
    "# initialize lists for storing test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# loop through the test data directory and extract the images and their labels\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(test_dir, category)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "            X_test.append(new_array)\n",
    "            y_test.append(class_num)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# convert test data to numpy arrays\n",
    "X_test = np.array(X_test).reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# normalize test data\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# calculate test accuracy\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4d62a",
   "metadata": {},
   "source": [
    "### CNN + HPO:\n",
    "-  ✔ Validation Accuracy: ~1.0000\n",
    "-  ✔ Test Accuracy: ~0.8429"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4ceed",
   "metadata": {},
   "source": [
    "### 6. Transfer Learning with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9688eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define image size\n",
    "img_size = 224\n",
    "\n",
    "# load the pre-trained model (VGG16)\n",
    "base_model = VGG16(input_shape=(img_size,img_size,3), include_top=False, weights='imagenet')\n",
    "\n",
    "# freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add custom layers for classification\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# create a new model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the image generators for training and validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# specify the training and validation data directories\n",
    "train_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\train'\n",
    "val_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\val'\n",
    "\n",
    "# create the image generators for training and validation data\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(img_size, img_size), batch_size=32, class_mode='binary')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=(img_size, img_size), batch_size=32, class_mode='binary')\n",
    "\n",
    "# train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
    "\n",
    "# evaluate the model on test data\n",
    "test_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\test'\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(img_size, img_size), batch_size=32, class_mode='binary', shuffle=False)\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8010d",
   "metadata": {},
   "source": [
    "### VGG16 Transfer Learning (Frozen):\n",
    "-  ✔ Validation Accuracy: ~0.9247\n",
    "-  ✔ Test Accuracy: ~0.9247\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe5892",
   "metadata": {},
   "source": [
    "### 7. Fine Tune VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ea8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten, Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "img_size=224\n",
    "base_model = VGG16(input_shape=(img_size,img_size,3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Unfreeze only last 4 layers\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# add custom layers for classification\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# create a new model\n",
    "model1 = Model(inputs=base_model.input, outputs=predictions)\n",
    "# define the image generators for training and validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# specify the training and validation data directories\n",
    "train_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\train'\n",
    "val_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\val'\n",
    "\n",
    "# create the image generators for training and validation data\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(img_size, img_size), batch_size=32, class_mode='binary')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=(img_size, img_size), batch_size=32, class_mode='binary')\n",
    "# compile the model\n",
    "model1.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model1.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)]\n",
    ")\n",
    "test_dir = r'C:\\Users\\rasik\\Downloads\\X-Ray Kaggle data\\chest_xray\\test'\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(img_size, img_size), batch_size=32, class_mode='binary', shuffle=False)\n",
    "test_loss, test_acc = model1.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125e136",
   "metadata": {},
   "source": [
    "### VGG16 Fine-Tuned:\n",
    "-   ✔ Validation Accuracy: 1.00\n",
    "-   ✔ Test Accuracy: 0.924\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910fb12",
   "metadata": {},
   "source": [
    "### 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ce764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"pneumonia_vgg16_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba2402",
   "metadata": {},
   "source": [
    "### 📈 Final Analysis Summary\n",
    " --------------------------------------------------\n",
    "\n",
    "### Model Comparisons:\n",
    " ------------------\n",
    "🔹 CNN from Scratch:\n",
    "    - Validation Accuracy: ~93.75%\n",
    "    - Test Accuracy: ~85.89%\n",
    "\n",
    "🔹 CNN + HPO:\n",
    "     - Validation Accuracy: 100.00% (overfitting suspected)\n",
    "     - Test Accuracy: ~84.29%\n",
    "\n",
    " 🔹 VGG16 Transfer Learning (Frozen):\n",
    "    - Validation Accuracy: ~92.47%\n",
    "    - Test Accuracy: ~92.47%\n",
    "\n",
    " 🔹 VGG16 Fine-Tuned:\n",
    "    - Validation Accuracy: 1.00\n",
    "    - Test Accuracy: 92.4%\n",
    "\n",
    " 📊 Insights:\n",
    " - HPO increased validation accuracy but may have overfit the data (test accuracy dropped).\n",
    " - Transfer learning using VGG16 gave the highest test performance without fine-tuning.\n",
    " - Fine-tuning is expected to further improve domain adaptation, though it increases training time.\n",
    " - Best balance of accuracy and generalization comes from using VGG16 with selective fine-tuning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (TensorFlow)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
